{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Baseline Implementation for SE125 Project 2\n",
    "\n",
    "We provide a baseline model for conversation modeling using deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries\n",
    "In this section, we import third-party libraries to be used in this project.\n",
    "You may need to install them using `pip`:\n",
    "```\n",
    "    pip install tqdm\n",
    "    pip install cython\n",
    "    pip install tables\n",
    "    pip install tensorboardX\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (4.46.1)\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting cython\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/19/49/91ebe4a00bf894d08dd9680cd9dfb05936eb2848eebd9402b43885aa74cf/Cython-0.29.21-cp36-cp36m-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: cython\n",
      "Successfully installed cython-0.29.21\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting tables\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ed/c3/8fd9e3bb21872f9d69eb93b3014c86479864cca94e625fd03713ccacec80/tables-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 989 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numexpr>=2.6.2\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/d1/72/43e0c31ac2315532586d32fc4c548d664fd4931b083642f1465ec15330d1/numexpr-2.7.2-cp36-cp36m-manylinux2010_x86_64.whl (469 kB)\n",
      "\u001b[K     |████████████████████████████████| 469 kB 21.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.3 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tables) (1.18.5)\n",
      "Installing collected packages: numexpr, tables\n",
      "Successfully installed numexpr-2.7.2 tables-3.6.1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting tensorboardX\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 701 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboardX) (3.12.2)\n",
      "Requirement already satisfied: six in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboardX) (1.15.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from tensorboardX) (1.18.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX) (47.3.1.post20200616)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.1\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting nltk\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 704 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 3.1 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from nltk) (0.15.1)\n",
      "Collecting regex\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/0d/8a/3ac62dadb767ace65a5b954265de4031a99b27148fe14b24771f5c2c2dca/regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "\u001b[K     |████████████████████████████████| 723 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (from nltk) (4.46.1)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434677 sha256=b112b3e19fe453bb03d2882086039ce774da44e9d3242eaa15b608c69a07dc33\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/87/0c/fc/bb36b0a8449de5bfd88193a03c30c91e3ff3f863106e61da85\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, regex, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2020.11.13\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install cython\n",
    "!pip install tables\n",
    "!pip install tensorboardX\n",
    "!pip install nltk\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import tables\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")#,format=\"%(asctime)s: %(name)s: %(levelname)s: %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilities\n",
    "\n",
    "In this section we maintain utilities for model construction and training. \n",
    "Please put your own utility modules/functions in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 111] Connection\n",
      "[nltk_data]     refused>\n"
     ]
    }
   ],
   "source": [
    "PAD_ID, SOS_ID, EOS_ID, UNK_ID = [0, 1, 2, 3]\n",
    "\n",
    "def asHHMMSS(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    h = math.floor(m /60)\n",
    "    m -= h *60\n",
    "    return '%d:%d:%d'% (h, m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s<%s'%(asHHMMSS(s), asHHMMSS(rs))\n",
    "\n",
    "#######################################################################\n",
    "import nltk\n",
    "try: \n",
    "    nltk.word_tokenize(\"hello world\")\n",
    "except LookupError: \n",
    "    nltk.download('punkt')\n",
    "    \n",
    "def sent2indexes(sentence, vocab, maxlen):\n",
    "    '''sentence: a string or list of string\n",
    "       return: a numpy array of word indices\n",
    "    '''      \n",
    "    def convert_sent(sent, vocab, maxlen):\n",
    "        idxes = np.zeros(maxlen, dtype=np.int64)\n",
    "        idxes.fill(PAD_ID)\n",
    "        tokens = nltk.word_tokenize(sent.strip())\n",
    "        idx_len = min(len(tokens), maxlen)\n",
    "        for i in range(idx_len): idxes[i] = vocab.get(tokens[i], UNK_ID)\n",
    "        return idxes, idx_len\n",
    "    if type(sentence) is list:\n",
    "        inds, lens = [], []\n",
    "        for sent in sentence:\n",
    "            idxes, idx_len = convert_sent(sent, vocab, maxlen)\n",
    "            #idxes, idx_len = np.expand_dims(idxes, 0), np.array([idx_len])\n",
    "            inds.append(idxes)\n",
    "            lens.append(idx_len)\n",
    "        return np.vstack(inds), np.vstack(lens)\n",
    "    else:\n",
    "        inds, lens = sent2indexes([sentence], vocab, maxlen)\n",
    "        return inds[0], lens[0]\n",
    "\n",
    "def indexes2sent(indexes, vocab, ignore_tok=PAD_ID): \n",
    "    '''indexes: numpy array'''\n",
    "    def revert_sent(indexes, ivocab, ignore_tok=PAD_ID):\n",
    "        toks=[]\n",
    "        length=0\n",
    "        indexes=filter(lambda i: i!=ignore_tok, indexes)\n",
    "        for idx in indexes:\n",
    "            toks.append(ivocab[idx])\n",
    "            length+=1\n",
    "            if idx == EOS_ID:\n",
    "                break\n",
    "        return ' '.join(toks), length\n",
    "    \n",
    "    ivocab = {v: k for k, v in vocab.items()}\n",
    "    if indexes.ndim==1:# one sentence\n",
    "        return revert_sent(indexes, ivocab, ignore_tok)\n",
    "    else:# dim>1\n",
    "        sentences=[] # a batch of sentences\n",
    "        lens=[]\n",
    "        for inds in indexes:\n",
    "            sentence, length = revert_sent(inds, ivocab, ignore_tok)\n",
    "            sentences.append(sentence)\n",
    "            lens.append(length)\n",
    "        return sentences, lens\n",
    "    \n",
    "def save_model(model, epoch):\n",
    "    \"\"\"Save model parameters to checkpoint\"\"\"\n",
    "    ckpt_path=f'./output/checkpoint_iter{epoch}.pkl'\n",
    "    #print(f'Saving model parameters to {ckpt_path}')\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "        \n",
    "def load_model(model, epoch):\n",
    "    \"\"\"Load parameters from checkpoint\"\"\"\n",
    "    ckpt_path=f'./output/checkpoint_iter{epoch}.pkl'\n",
    "    #print(f'Loading model parameters from {ckpt_path}')\n",
    "    model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "In this section, we configurate some hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    conf = {\n",
    "    'maxlen':40, # maximum utterance length\n",
    "    'diaglen':10, # how many utterance kept in the context window\n",
    "\n",
    "    # Model Arguments\n",
    "    'emb_size':200, # size of word embeddings\n",
    "    'rnn_hid_utt':512, # number of rnn hidden units for utterance encoder\n",
    "    'rnn_hid_ctx':512, # number of rnn hidden units for context encoder\n",
    "    'rnn_hid_dec':512, # number of rnn hidden units for decoder\n",
    "    'n_layers':1, # number of layers\n",
    "    'dropout':0.5, # dropout applied to layers (0 = no dropout)\n",
    "    'teach_force': 0.8, # use teach force for decoder\n",
    "      \n",
    "    # Training Arguments\n",
    "    'batch_size':64,\n",
    "    'epochs':10, # maximum number of epochs\n",
    "    'lr':2e-4, # autoencoder learning rate\n",
    "    'beta1':0.9, # beta1 for adam\n",
    "    'init_w':0.05, # initial w\n",
    "    'clip':5.0,  # gradient clipping, max norm        \n",
    "    }\n",
    "    return conf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loader\n",
    "A tool to load batches from the binarized (.h5) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(data.Dataset):\n",
    "    def __init__(self, filepath, max_ctx_len=7, max_utt_len=40):\n",
    "        # 1. Initialize file path or list of file names.\n",
    "        \"\"\"read training sentences(list of int array) from a hdf5 file\"\"\"\n",
    "        self.max_ctx_len=max_ctx_len\n",
    "        self.max_utt_len=max_utt_len\n",
    "        \n",
    "        print(\"loading data...\")\n",
    "        table = tables.open_file(filepath)\n",
    "        self.data = table.get_node('/sentences')[:].astype(np.long)\n",
    "        self.index = table.get_node('/indices')[:]\n",
    "        self.data_len = self.index.shape[0]\n",
    "        print(\"{} entries\".format(self.data_len))\n",
    "\n",
    "    def __getitem__(self, offset):\n",
    "        pos_utt, ctx_len, res_len = self.index[offset]['pos_utt'], self.index[offset]['ctx_len'], self.index[offset]['res_len']\n",
    "        ctx_arr=self.data[pos_utt-ctx_len:pos_utt]\n",
    "        res_arr=self.data[pos_utt:pos_utt+res_len]\n",
    "        ## split context array into utterances\n",
    "        context=[]\n",
    "        utt_lens=[]\n",
    "        utt=[]\n",
    "        for i, tok in enumerate(ctx_arr):\n",
    "            utt.append(ctx_arr[i])\n",
    "            if tok==EOS_ID:\n",
    "                if len(utt)<self.max_utt_len+1:\n",
    "                    utt_lens.append(len(utt)-1)# floor is not counted in the utt length\n",
    "                    utt.extend([PAD_ID]*(self.max_utt_len+1-len(utt)))  \n",
    "                else:\n",
    "                    utt=utt[:self.max_utt_len+1]\n",
    "                    utt[-1]=EOS_ID\n",
    "                    utt_lens.append(self.max_utt_len)\n",
    "                context.append(utt)                \n",
    "                utt=[]    \n",
    "        if len(context)>self.max_ctx_len: # trunk long context\n",
    "            context=context[-self.max_ctx_len:]\n",
    "            utt_lens=utt_lens[-self.max_ctx_len:]\n",
    "        context_len=len(context)\n",
    "        \n",
    "        if len(context)<self.max_ctx_len: # pad short context\n",
    "            for i in range(len(context), self.max_ctx_len):\n",
    "                context.append([0, SOS_ID, EOS_ID]+[PAD_ID]*(self.max_utt_len-2)) # [floor, <sos>, <eos>, <pad>, <pad> ...]\n",
    "                utt_lens.append(2) # <s> and </s>\n",
    "        context = np.array(context)        \n",
    "        utt_lens=np.array(utt_lens)\n",
    "        floors=context[:,0]\n",
    "        context = context[:,1:]\n",
    "        \n",
    "        ## Padding ##    \n",
    "        response = res_arr[1:]\n",
    "        if len(response)<self.max_utt_len:\n",
    "            res_len=len(response)\n",
    "            response=np.append(response,[PAD_ID]*(self.max_utt_len-len(response)))\n",
    "        else:\n",
    "            response=response[:self.max_utt_len]\n",
    "            response[-1]=EOS_ID\n",
    "            res_len=self.max_utt_len\n",
    "\n",
    "        return context, context_len, utt_lens, floors, response, res_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "\n",
    "def load_dict(filename):\n",
    "    return json.loads(open(filename, \"r\").readline())\n",
    "\n",
    "def load_vecs(fin):         \n",
    "    \"\"\"read vectors (2D numpy array) from a hdf5 file\"\"\"\n",
    "    h5f = tables.open_file(fin)\n",
    "    h5vecs= h5f.root.vecs\n",
    "    \n",
    "    vecs=np.zeros(shape=h5vecs.shape,dtype=h5vecs.dtype)\n",
    "    vecs[:]=h5vecs[:]\n",
    "    h5f.close()\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Models\n",
    "Define your model(including its dependent sub-modules) here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as weight_init\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, embedder, input_size, hidden_size, bidir, n_layers, dropout=0.5):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.bidir = bidir\n",
    "        assert type(self.bidir)==bool\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        self.embedding = embedder # nn.Embedding(vocab_size, emb_size)\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, n_layers, batch_first=True, bidirectional=bidir)\n",
    "        self.init_h = nn.Parameter(torch.randn(self.n_layers*(1+self.bidir), 1, self.hidden_size), requires_grad=True)#learnable h0\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"adopted from https://gist.github.com/jeasinema/ed9236ce743c8efaf30fa2ff732749f5\"\"\"\n",
    "        for w in self.rnn.parameters(): # initialize the gate weights with orthogonal\n",
    "            if len(w.shape)>1: \n",
    "                weight_init.orthogonal_(w.data)\n",
    "            else:\n",
    "                weight_init.normal_(w.data)\n",
    "                \n",
    "    \n",
    "    def forward(self, inputs, input_lens=None, init_h=None): \n",
    "        # init_h: [n_layers*n_dir x batch_size x hid_size]\n",
    "        if self.embedding is not None:\n",
    "            inputs=self.embedding(inputs)  # input: [batch_sz x seq_len] -> [batch_sz x seq_len x emb_sz]\n",
    "        \n",
    "        batch_size, seq_len, emb_size=inputs.size()\n",
    "        inputs=F.dropout(inputs, self.dropout, self.training)# dropout\n",
    "        \n",
    "        if input_lens is not None:# sort and pack sequence \n",
    "            input_lens_sorted, indices = input_lens.sort(descending=True)\n",
    "            inputs_sorted = inputs.index_select(0, indices)        \n",
    "            inputs = pack_padded_sequence(inputs_sorted, input_lens_sorted.data.tolist(), batch_first=True)\n",
    "        \n",
    "        if init_h is None:\n",
    "            init_h = self.init_h.expand(-1,batch_size,-1).contiguous()# use learnable initial states, expanding along batches\n",
    "        #self.rnn.flatten_parameters() # time consuming!!\n",
    "        hids, h_n = self.rnn(inputs, init_h) # hids: [b x seq x (n_dir*hid_sz)]  \n",
    "                                                  # h_n: [(n_layers*n_dir) x batch_sz x hid_sz] (2=fw&bw)\n",
    "        if input_lens is not None: # reorder and pad\n",
    "            _, inv_indices = indices.sort()\n",
    "            hids, lens = pad_packed_sequence(hids, batch_first=True)     \n",
    "            hids = hids.index_select(0, inv_indices)\n",
    "            h_n = h_n.index_select(1, inv_indices)\n",
    "        h_n = h_n.view(self.n_layers, (1+self.bidir), batch_size, self.hidden_size) #[n_layers x n_dirs x batch_sz x hid_sz]\n",
    "        h_n = h_n[-1] # get the last layer [n_dirs x batch_sz x hid_sz]\n",
    "        enc = h_n.view(batch_size,-1) #[batch_sz x (n_dirs*hid_sz)]\n",
    "            \n",
    "        return enc, hids\n",
    "    \n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self, utt_encoder, input_size, hidden_size, n_layers=1, dropout=0.5):\n",
    "        super(ContextEncoder, self).__init__()     \n",
    "        self.utt_encoder=utt_encoder\n",
    "        self.ctx_encoder= RNNEncoder(None, input_size, hidden_size, False, n_layers, dropout)\n",
    "\n",
    "    def forward(self, context, context_lens, utt_lens): # context: [batch_sz x diag_len x max_utt_len] \n",
    "                                                      # context_lens: [batch_sz x dia_len]\n",
    "        batch_size, max_context_len, max_utt_len = context.size()\n",
    "        utts = context.view(-1, max_utt_len) # [(batch_size*diag_len) x max_utt_len]\n",
    "        utt_lens = utt_lens.view(-1)\n",
    "        utt_encs, _ = self.utt_encoder(utts, utt_lens) # [(batch_size*diag_len) x 2hid_size]\n",
    "        \n",
    "        utt_encs = utt_encs.view(batch_size, max_context_len, -1)\n",
    "        enc, hids = self.ctx_encoder(utt_encs, context_lens)\n",
    "        return enc\n",
    "  \n",
    "\n",
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, embedder, input_size, hidden_size, vocab_size, n_layers=1, dropout=0.5):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size= input_size # size of the input to the RNN (e.g., embedding dim)\n",
    "        self.hidden_size = hidden_size # RNN hidden size\n",
    "        self.vocab_size = vocab_size # RNN output size (vocab size)\n",
    "        self.dropout= dropout\n",
    "        \n",
    "        self.embedding = embedder\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.project = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for w in self.rnn.parameters(): # initialize the gate weights with orthogonal\n",
    "            if w.dim()>1:\n",
    "                weight_init.orthogonal_(w)\n",
    "        self.project.weight.data.uniform_(-0.1, 0.1)#nn.init.xavier_normal_(self.out.weight)        \n",
    "        nn.init.constant_(self.project.bias, 0.)\n",
    "\n",
    "    def forward(self, init_h, inputs=None, lens=None, enc_hids=None, src_pad_mask=None, context=None):\n",
    "        '''\n",
    "        init_h: initial hidden state for decoder\n",
    "        enc_hids: enc_hids for attention use\n",
    "        context: context information to be paired with input\n",
    "        inputs: inputs to the decoder\n",
    "        lens: input lengths\n",
    "        '''\n",
    "        if self.embedding is not None:\n",
    "            inputs = self.embedding(inputs) # input: [batch_sz x seqlen x emb_sz]\n",
    "        batch_size, maxlen, _ = inputs.size()\n",
    "        inputs = F.dropout(inputs, self.dropout, self.training)  \n",
    "        h = init_h.unsqueeze(0) # last_hidden of decoder [n_dir x batch_sz x hid_sz]        \n",
    "\n",
    "        if context is not None:            \n",
    "            repeated_context = context.unsqueeze(1).repeat(1, maxlen, 1) # [batch_sz x max_len x hid_sz]\n",
    "            inputs = torch.cat([inputs, repeated_context], 2)\n",
    "                \n",
    "            #self.rnn.flatten_parameters()\n",
    "        hids, h = self.rnn(inputs, h)         \n",
    "        decoded = self.project(hids.contiguous().view(-1, self.hidden_size))# reshape before linear over vocab\n",
    "        decoded = decoded.view(batch_size, maxlen, self.vocab_size)\n",
    "        return decoded, h\n",
    "    \n",
    "    def sampling(self, init_h, enc_hids, src_pad_mask, context, maxlen, to_numpy=True):\n",
    "        \"\"\"\n",
    "        A simple greedy sampling\n",
    "        :param init_h: [batch_sz x hid_sz]\n",
    "        :param enc_hids: a tuple of (enc_hids, mask) for attention use. [batch_sz x seq_len x hid_sz]\n",
    "        \"\"\"\n",
    "        device = init_h.device\n",
    "        batch_size = init_h.size(0)\n",
    "        decoded_words = torch.zeros((batch_size, maxlen), dtype=torch.long, device=device)  \n",
    "        sample_lens = torch.zeros((batch_size), dtype=torch.long, device=device)\n",
    "        len_inc = torch.ones((batch_size), dtype=torch.long, device=device)\n",
    "               \n",
    "        x = torch.zeros((batch_size, 1), dtype=torch.long, device=device).fill_(SOS_ID)# [batch_sz x 1] (1=seq_len)\n",
    "        h = init_h.unsqueeze(0) # [1 x batch_sz x hid_sz]  \n",
    "        for di in range(maxlen):  \n",
    "            if self.embedding is not None:\n",
    "                x = self.embedding(x) # x: [batch_sz x 1 x emb_sz]\n",
    "            h_n, h = self.rnn(x, h) # h_n: [batch_sz x 1 x hid_sz] h: [1 x batch_sz x hid_sz]\n",
    "\n",
    "            logits = self.project(h_n) # out: [batch_sz x 1 x vocab_sz]  \n",
    "            logits = logits.squeeze(1) # [batch_size x vocab_size]                  \n",
    "            x = torch.multinomial(F.softmax(logits, dim=1), 1)  # [batch_size x 1 x 1]?\n",
    "            decoded_words[:,di] = x.squeeze()\n",
    "            len_inc=len_inc*(x.squeeze()!=EOS_ID).long() # stop increse length (set 0 bit) when EOS is met\n",
    "            sample_lens=sample_lens+len_inc            \n",
    "        \n",
    "        if to_numpy:\n",
    "            decoded_words = decoded_words.data.cpu().numpy()\n",
    "            sample_lens = sample_lens.data.cpu().numpy()\n",
    "        return decoded_words, sample_lens\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    '''The basic Hierarchical Recurrent Encoder-Decoder model. '''\n",
    "    def __init__(self, config, vocab_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.vocab_size = vocab_size \n",
    "        self.maxlen=config['maxlen']\n",
    "        self.clip = config['clip']\n",
    "        self.init_w = config['init_w']\n",
    "        \n",
    "        self.embedder= nn.Embedding(vocab_size, config['emb_size'], padding_idx=PAD_ID)\n",
    "        self.utt_encoder = RNNEncoder(self.embedder, config['emb_size'], config['rnn_hid_utt'], True, \n",
    "                                   config['n_layers'], config['dropout']) \n",
    "                                                        # utter encoder: encode response to vector\n",
    "        self.context_encoder = ContextEncoder(self.utt_encoder, config['rnn_hid_utt']*2,\n",
    "                                              config['rnn_hid_ctx'], 1, config['dropout']) \n",
    "                                              # context encoder: encode context to vector    \n",
    "        self.decoder = RNNDecoder(self.embedder, config['emb_size'], config['rnn_hid_ctx'], vocab_size, 1, config['dropout']) # utter decoder: P(x|c,z)\n",
    "        self.optimizer = optim.Adam(list(self.context_encoder.parameters())\n",
    "                                      +list(self.decoder.parameters()),lr=config['lr'])\n",
    "\n",
    "    def forward(self, context, context_lens, utt_lens, response, res_lens):\n",
    "        c = self.context_encoder(context, context_lens, utt_lens)\n",
    "        output,_ = self.decoder(c, response[:,:-1], res_lens-1) # decode from z, c  # output: [batch x seq_len x n_tokens]   \n",
    "        dec_target = response[:,1:].clone()\n",
    "        dec_target[response[:,1:]==PAD_ID] = -100\n",
    "        loss = nn.CrossEntropyLoss()(output.view(-1, self.vocab_size), dec_target.view(-1))\n",
    "        return loss\n",
    "    \n",
    "    def train_batch(self, context, context_lens, utt_lens, response, res_lens):\n",
    "        self.context_encoder.train()\n",
    "        self.decoder.train()\n",
    "        \n",
    "        loss = self.forward(context, context_lens, utt_lens, response, res_lens)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` to prevent exploding gradient in RNNs\n",
    "        nn.utils.clip_grad_norm_(list(self.context_encoder.parameters())+list(self.decoder.parameters()), self.clip)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {'train_loss': loss.item()}      \n",
    "    \n",
    "    def valid(self, context, context_lens, utt_lens, response, res_lens):\n",
    "        self.context_encoder.eval()  \n",
    "        self.decoder.eval()        \n",
    "        loss = self.forward(context, context_lens, utt_lens, response, res_lens)\n",
    "        return {'valid_loss': loss.item()}\n",
    "    \n",
    "    def sample(self, context, context_lens, utt_lens, n_samples):    \n",
    "        self.context_encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            c = self.context_encoder(context, context_lens, utt_lens)\n",
    "        sample_words, sample_lens = self.decoder.sampling(c, None, None, None, n_samples, self.maxlen)  \n",
    "        return sample_words, sample_lens  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "We provide the evaluation script as well as the BLEU score metric. \n",
    "\n",
    "**Do not change code in this block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from collections import Counter\n",
    "\n",
    "class Metrics:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Metrics, self).__init__()\n",
    "\n",
    "    def sim_bleu(self, hyps, ref):\n",
    "        \"\"\"\n",
    "        :param ref - a list of tokens of the reference\n",
    "        :param hyps - a list of tokens of the hypothesis\n",
    "    \n",
    "        :return maxbleu - recall bleu\n",
    "        :return avgbleu - precision bleu\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for hyp in hyps:\n",
    "            try:\n",
    "                scores.append(sentence_bleu([ref], hyp, smoothing_function=SmoothingFunction().method7,\n",
    "                                        weights=[1./4, 1./4, 1./4, 1./4]))\n",
    "            except:\n",
    "                scores.append(0.0)\n",
    "        return np.max(scores), np.mean(scores)\n",
    "    \n",
    "def evaluate(model, metrics, test_loader, vocab, repeat, f_eval):\n",
    "    ivocab = {v: k for k, v in vocab.items()}\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    recall_bleus, prec_bleus, avg_lens  = [], [], []\n",
    "        \n",
    "    dlg_id = 0\n",
    "    for context, context_lens, utt_lens, floors, response, res_lens in tqdm(test_loader): \n",
    "        \n",
    "        if dlg_id > 5000: break\n",
    "        \n",
    "#        max_ctx_len = max(context_lens)\n",
    "        max_ctx_len = context.size(1)\n",
    "        context, utt_lens, floors = context[:,:max_ctx_len,1:], utt_lens[:,:max_ctx_len]-1, floors[:,:max_ctx_len] \n",
    "                         # remove empty utts and the sos token in the context and reduce the context length\n",
    "        ctx, ctx_lens = context, context_lens\n",
    "        context, context_lens, utt_lens \\\n",
    "            = [tensor.to(device) for tensor in [context, context_lens, utt_lens]]\n",
    "\n",
    "#################################################\n",
    "        utt_lens[utt_lens<=0]=1\n",
    "#################################################\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sample_words, sample_lens = model.sample(context, context_lens, utt_lens, repeat)\n",
    "        # nparray: [repeat x seq_len]       \n",
    "        \n",
    "        pred_sents, _ = indexes2sent(sample_words, vocab)\n",
    "        pred_tokens = [sent.split(' ') for sent in pred_sents]   \n",
    "        ref_str, _ =indexes2sent(response[0].numpy(), vocab, SOS_ID)\n",
    "        #ref_str = ref_str.encode('utf-8')\n",
    "        ref_tokens = ref_str.split(' ')\n",
    "        \n",
    "        max_bleu, avg_bleu = metrics.sim_bleu(pred_tokens, ref_tokens)\n",
    "        recall_bleus.append(max_bleu)\n",
    "        prec_bleus.append(avg_bleu)\n",
    "        \n",
    "        avg_lens.append(np.mean(sample_lens))\n",
    "\n",
    "        response, res_lens = [tensor.to(device) for tensor in [response, res_lens]]\n",
    "        \n",
    "        ## Write concrete results to a text file\n",
    "        dlg_id += 1 \n",
    "        if f_eval is not None:\n",
    "            f_eval.write(\"Batch {:d} \\n\".format(dlg_id))\n",
    "            # print the context\n",
    "            start = np.maximum(0, ctx_lens[0]-5)\n",
    "            for t_id in range(start, ctx_lens[0], 1):\n",
    "                context_str = indexes2sent(ctx[0, t_id].numpy(), vocab)\n",
    "                f_eval.write(\"Context {:d}-{:d}: {}\\n\".format(t_id, floors[0, t_id], context_str))\n",
    "            #print the ground truth response    \n",
    "            f_eval.write(\"Target >> {}\\n\".format(ref_str.replace(\" ' \", \"'\")))\n",
    "            for res_id, pred_sent in enumerate(pred_sents):\n",
    "                f_eval.write(\"Sample {:d} >> {}\\n\".format(res_id, pred_sent.replace(\" ' \", \"'\")))\n",
    "            f_eval.write(\"\\n\")\n",
    "    prec_bleu= float(np.mean(prec_bleus))\n",
    "    recall_bleu = float(np.mean(recall_bleus))\n",
    "    result = {'avg_len':float(np.mean(avg_lens)),\n",
    "              'recall_bleu': recall_bleu, 'prec_bleu': prec_bleu, \n",
    "              'f1_bleu': 2*(prec_bleu*recall_bleu) / (prec_bleu+recall_bleu+10e-12),\n",
    "             }\n",
    "    \n",
    "    if f_eval is not None:\n",
    "        for k, v in result.items():\n",
    "            f_eval.write(str(k) + ':'+ str(v)+' ')\n",
    "        f_eval.write('\\n')\n",
    "    print(\"Done testing\")\n",
    "    print(result)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training\n",
    "The training script here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "from tensorboardX import SummaryWriter # install tensorboardX (pip install tensorboardX) before importing this package\n",
    "\n",
    "def train(args, model=None, pad = 0):\n",
    "    # LOG #\n",
    "    fh = logging.FileHandler(f\"./output/logs.txt\")\n",
    "                                      # create file handler which logs even debug messages\n",
    "    logger.addHandler(fh)# add the handlers to the logger\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d%H%M')\n",
    "    tb_writer = SummaryWriter(f\"./output/logs/{timestamp}\") if args.visual else None\n",
    "\n",
    "    # Set the random seed manually for reproducibility.\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    device = torch.device(f\"cuda:{args.gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "\n",
    "    config=get_config()\n",
    "\n",
    "    if args.visual:\n",
    "        json.dump(config, open(f'./output/config_{timestamp}.json', 'w'))# save configs\n",
    "\n",
    "    ###############################################################################\n",
    "    # Load data\n",
    "    ###############################################################################\n",
    "    data_path = args.data_path+args.dataset+'/'\n",
    "    train_set = DialogDataset(os.path.join(data_path, 'train.h5'), config['diaglen'], config['maxlen'])\n",
    "    valid_set = DialogDataset(os.path.join(data_path, 'valid.h5'), config['diaglen'], config['maxlen'])\n",
    "    test_set = DialogDataset(os.path.join(data_path, 'test.h5'), config['diaglen'], config['maxlen'])\n",
    "    vocab = load_dict(os.path.join(data_path, 'vocab.json'))\n",
    "    ivocab = {v: k for k, v in vocab.items()}\n",
    "    n_tokens = len(ivocab)\n",
    "    metrics=Metrics()    \n",
    "    print(\"Loaded data!\")\n",
    "\n",
    "    ###############################################################################\n",
    "    # Define the models\n",
    "    ###############################################################################\n",
    "    if model is None:\n",
    "        model = MyModel(config, n_tokens)\n",
    "\n",
    "    if args.reload_from>=0:\n",
    "        load_model(model, args.reload_from)\n",
    "        \n",
    "    model=model.to(device)\n",
    "\n",
    "    logger.info(\"Training...\")\n",
    "    best_perf = -1\n",
    "    itr_global=1\n",
    "    start_epoch=1 if args.reload_from==-1 else args.reload_from+1\n",
    "    for epoch in range(start_epoch, config['epochs']+1):\n",
    "        epoch_start_time = time.time()\n",
    "        itr_start_time = time.time()\n",
    "        \n",
    "        # shuffle (re-define) data between epochs   \n",
    "        train_loader=torch.utils.data.DataLoader(dataset=train_set, batch_size=config['batch_size'],\n",
    "                                                 shuffle=True, num_workers=1, drop_last=True)\n",
    "        n_iters=train_loader.__len__()\n",
    "        itr = 1\n",
    "        for batch in train_loader:# loop through all batches in training data\n",
    "            model.train()\n",
    "            context, context_lens, utt_lens, floors, response, res_lens = batch\n",
    "\n",
    " #           max_ctx_len = max(context_lens)\n",
    "            max_ctx_len = context.size(1)\n",
    "            context, utt_lens = context[:,:max_ctx_len,1:], utt_lens[:,:max_ctx_len]-1\n",
    "                                    # remove empty utterances in context\n",
    "                                    # remove the sos token in the context and reduce the context length     \n",
    "#################################################\n",
    "            utt_lens[utt_lens<=0]=1\n",
    "#################################################\n",
    "            batch_gpu = [tensor.to(device) for tensor in [context, context_lens, utt_lens, response, res_lens]] \n",
    "            train_results = model.train_batch(*batch_gpu)\n",
    "                     \n",
    "            if itr % args.log_every == 0:\n",
    "                elapsed = time.time() - itr_start_time\n",
    "                log = '%s|%s@gpu%d epo:[%d/%d] iter:[%d/%d] step_time:%ds elapsed:%s'\\\n",
    "                %(args.model, args.dataset, args.gpu_id, epoch, config['epochs'],\n",
    "                         itr, n_iters, elapsed, timeSince(epoch_start_time,itr/n_iters))\n",
    "                logger.info(log)\n",
    "                logger.info(train_results)\n",
    "                if args.visual:\n",
    "                    tb_writer.add_scalar('train_loss', train_results['train_loss'], itr_global)\n",
    "\n",
    "                itr_start_time = time.time()    \n",
    "                \n",
    "            if itr % args.valid_every == 0 and False:\n",
    "                logger.info('Validation ')\n",
    "                valid_loader=torch.utils.data.DataLoader(dataset=valid_set, batch_size=config['batch_size'], shuffle=True, num_workers=1)\n",
    "                model.eval()    \n",
    "                valid_losses = []\n",
    "                for context, context_lens, utt_lens, floors, response, res_lens in valid_loader:\n",
    " #                   max_ctx_len = max(context_lens)\n",
    "                    max_ctx_len = context.size(1)\n",
    "                    context, utt_lens = context[:,:max_ctx_len,1:], utt_lens[:,:max_ctx_len]-1\n",
    "                             # remove empty utterances in context\n",
    "                             # remove the sos token in the context and reduce the context length\n",
    "#################################################\n",
    "                    utt_lens[utt_lens<=0]=1\n",
    "#################################################\n",
    "                    batch = [tensor.to(device) for tensor in [context, context_lens, utt_lens, response, res_lens]]\n",
    "                    valid_results = model.valid(*batch)    \n",
    "                    valid_losses.append(valid_results['valid_loss'])\n",
    "                if args.visual: tb_writer.add_scalar('valid_loss', np.mean(valid_losses), itr_global)\n",
    "                logger.info({'valid_loss':np.mean(valid_losses)})    \n",
    "                \n",
    "            itr += 1\n",
    "            itr_global+=1            \n",
    "            \n",
    "            if itr_global % args.eval_every == 0:  # evaluate the model in the validation set\n",
    "                model.eval()          \n",
    "                logger.info(\"Evaluating in the validation set..\")\n",
    "\n",
    "                valid_loader=torch.utils.data.DataLoader(dataset=valid_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "                f_eval = open(f\"./output/tmp_results/iter{itr_global}.txt\", \"w\")\n",
    "                repeat = 10            \n",
    "                eval_results = evaluate(model, metrics, valid_loader, vocab, repeat, f_eval)\n",
    "                bleu = eval_results['recall_bleu']\n",
    "                if bleu> best_perf:\n",
    "                    save_model(model, 0)#itr_global) # save model after each epoch\n",
    "                if args.visual:\n",
    "                    tb_writer.add_scalar('recall_bleu', bleu, itr_global)\n",
    "                \n",
    "        # end of epoch ----------------------------\n",
    "               # model.adjust_lr()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Main Function (Training)\n",
    "You can change the default arguments by setting the `default` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': './data/', 'model': 'MyModel', 'dataset': 'dailydialog', 'visual': False, 'reload_from': -1, 'gpu_id': 0, 'log_every': 100, 'valid_every': 1000, 'eval_every': 2000, 'seed': 1111}\n",
      "cuda:0\n",
      "loading data...\n",
      "76052 entries\n",
      "loading data...\n",
      "7069 entries\n",
      "loading data...\n",
      "7069 entries\n",
      "Loaded data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[100/1188] step_time:5s elapsed:0:0:5<0:1:2\n",
      "{'train_loss': 5.200043678283691}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-628e38d80842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# fix the random seed in cudnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-3ba09267bafb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, pad)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m#################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mbatch_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutt_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_lens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6842937a8f97>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, context, context_lens, utt_lens, response, res_lens)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# `clip_grad_norm` to prevent exploding gradient in RNNs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Dialog Pytorch')\n",
    "    # Path Arguments\n",
    "    parser.add_argument('--data_path', type=str, default='./data/', help='location of the data corpus')\n",
    "    parser.add_argument('--model', type=str, default='MyModel', help='model name')\n",
    "    parser.add_argument('--dataset', type=str, default='dailydialog', help='name of dataset.')\n",
    "    parser.add_argument('-v','--visual', action='store_true', default=False, help='visualize training status in tensorboard')\n",
    "    parser.add_argument('--reload_from', type=int, default=-1, help='reload from a trained ephoch')\n",
    "    parser.add_argument('--gpu_id', type=int, default=0, help='GPU ID')\n",
    "\n",
    "    # Evaluation Arguments\n",
    "    parser.add_argument('--log_every', type=int, default=100, help='interval to log autoencoder training results')\n",
    "    parser.add_argument('--valid_every', type=int, default=1000, help='interval to validation')\n",
    "    parser.add_argument('--eval_every', type=int, default=2000, help='interval to evaluation to concrete results')\n",
    "    parser.add_argument('--seed', type=int, default=1111, help='random seed')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "    print(vars(args))\n",
    "\n",
    "    # make output directory if it doesn't already exist\n",
    "    os.makedirs(f'./output/models', exist_ok=True)\n",
    "    os.makedirs(f'./output/tmp_results', exist_ok=True)\n",
    "        \n",
    "    torch.backends.cudnn.benchmark = True # speed up training by using cudnn\n",
    "    torch.backends.cudnn.deterministic = True # fix the random seed in cudnn\n",
    "    \n",
    "    model = train(args)\n",
    "    \n",
    "    start = time.clock()\n",
    "    model = train(args)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print(\"Time used:\",elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Function (Test)\n",
    "\n",
    "**Please do not change code here except the default arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args):\n",
    "    conf = get_config()\n",
    "    # Set the random seed manually for reproducibility.\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    else:\n",
    "        print(\"Note that our pre-trained models require CUDA to evaluate.\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path=args.data_path+args.dataset+'/'\n",
    "    test_set=DialogDataset(data_path+'test.h5', conf['diaglen'], conf['maxlen'])\n",
    "    test_loader=torch.utils.data.DataLoader(dataset=test_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "    vocab = load_dict(data_path+'vocab.json')\n",
    "    n_tokens = len(vocab)\n",
    "\n",
    "    metrics=Metrics()\n",
    "    \n",
    "    # Load model checkpoints    \n",
    "    model = MyModel(conf, n_tokens)\n",
    "    load_model(model, 0)\n",
    "    #model=model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    f_eval = open(\"./output/results.txt\", \"w\")\n",
    "    repeat = args.n_samples\n",
    "    \n",
    "    evaluate(model, metrics, test_loader, vocab, repeat, f_eval)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='PyTorch DialogGAN for Eval')\n",
    "    parser.add_argument('--data_path', type=str, default='./data/', help='location of the data corpus')\n",
    "    parser.add_argument('--dataset', type=str, default='dailydialog', help='name of dataset, SWDA or DailyDial')\n",
    "    parser.add_argument('--model', type=str, default='MyModel', help='model name')\n",
    "    parser.add_argument('--reload_from', type=int, default=0, \n",
    "                        help='directory to load models from')\n",
    "    \n",
    "    parser.add_argument('--n_samples', type=int, default=10, help='Number of responses to sampling')\n",
    "    parser.add_argument('--seed', type=int, default=1111, help='random seed')\n",
    "    args = parser.parse_args(args=[])\n",
    "    print(vars(args))\n",
    "    test(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. transformer implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "# Multi-Head Attention module\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        # Transpose for attention dot product: b x n x lq x dv\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "        q, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q, attn\n",
    "\n",
    "# A two-feed-forward-layer module\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n",
    "        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    ''' Compose with two layers '''\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, slf_attn_mask=None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output, enc_slf_attn\n",
    "\n",
    "# Compose with three layers\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(\n",
    "            self, dec_input, enc_output,\n",
    "            slf_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        dec_output, dec_slf_attn = self.slf_attn(\n",
    "            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n",
    "        dec_output, dec_enc_attn = self.enc_attn(\n",
    "            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n",
    "        dec_output = self.pos_ffn(dec_output)\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn\n",
    "\n",
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "# For masking out the subsequent info\n",
    "def get_subsequent_mask(seq):\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = (1 - torch.triu(\n",
    "        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_hid, n_position=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Not a parameter\n",
    "        # 定义一个不是模型参数的固定缓冲区\n",
    "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n",
    "    # Sinusoid position encoding table\n",
    "    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n",
    "\n",
    "        def get_position_angle_vec(position):\n",
    "            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "        # pos/10000^(2i/d_moudle)\n",
    "\n",
    "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i 偶数sin\n",
    "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1 奇数cos\n",
    "\n",
    "        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()\n",
    "\n",
    "# A encoder model with self attention mechanism\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, pad_idx, dropout=0.1, n_position=200):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx) # 词向量化\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position) # 位置信息层\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)]) # 多层encoder组合\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6) # 归一化层\n",
    "\n",
    "    def forward(self, src_seq, src_mask, return_attns=False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        # -- Forward\n",
    "        \n",
    "        enc_output = self.dropout(self.position_enc(self.src_word_emb(src_seq))) # 词向量化之后加入位置向量再dropout\n",
    "        enc_output = self.layer_norm(enc_output) # 归一化数据\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask) # 逐层计算\n",
    "            enc_slf_attn_list += [enc_slf_attn] if return_attns else [] # 如果需要self attention信息，每层保存\n",
    "\n",
    "        if return_attns:\n",
    "            return enc_output, enc_slf_attn_list\n",
    "        return enc_output,\n",
    "\n",
    "# A decoder model with self attention mechanism\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, pad_idx, n_position=200, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns=False):\n",
    "\n",
    "        dec_slf_attn_list, dec_enc_attn_list = [], []\n",
    "\n",
    "        # -- Forward\n",
    "        dec_output = self.dropout(self.position_enc(self.trg_word_emb(trg_seq)))\n",
    "        dec_output = self.layer_norm(dec_output)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n",
    "            dec_slf_attn_list += [dec_slf_attn] if return_attns else []\n",
    "            dec_enc_attn_list += [dec_enc_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return dec_output,\n",
    "\n",
    "# A sequence to sequence model with attention mechanism\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,\n",
    "            d_word_vec=512, d_model=512, d_inner=2048,\n",
    "            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1, n_position=200,\n",
    "            trg_emb_prj_weight_sharing=True, emb_src_trg_weight_sharing=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab=n_src_vocab, n_position=n_position,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            pad_idx=src_pad_idx, dropout=dropout)\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            n_trg_vocab=n_trg_vocab, n_position=n_position,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            pad_idx=trg_pad_idx, dropout=dropout)\n",
    "\n",
    "        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p) \n",
    "\n",
    "        assert d_model == d_word_vec, \\\n",
    "        'To facilitate the residual connections, \\\n",
    "         the dimensions of all module outputs shall be the same.'\n",
    "\n",
    "        self.x_logit_scale = 1.\n",
    "        if trg_emb_prj_weight_sharing:\n",
    "            # Share the weight between target word embedding & last dense layer\n",
    "            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight\n",
    "            self.x_logit_scale = (d_model ** -0.5)\n",
    "\n",
    "        if emb_src_trg_weight_sharing:\n",
    "            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight\n",
    "\n",
    "\n",
    "    def forward(self, src_seq, trg_seq):\n",
    "\n",
    "        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n",
    "        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) & get_subsequent_mask(trg_seq)\n",
    "\n",
    "        enc_output, *_ = self.encoder(src_seq, src_mask)\n",
    "        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)\n",
    "        seq_logit = self.trg_word_prj(dec_output) * self.x_logit_scale\n",
    "\n",
    "        return seq_logit.view(-1, seq_logit.size(2))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Apply label smoothing if needed\n",
    "def cal_performance(pred, gold, trg_pad_idx, smoothing=False):\n",
    "\n",
    "    loss = cal_loss(pred, gold, trg_pad_idx, smoothing=smoothing)\n",
    "\n",
    "    pred = pred.max(1)[1]\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    non_pad_mask = gold.ne(trg_pad_idx)\n",
    "    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n",
    "    n_word = non_pad_mask.sum().item()\n",
    "\n",
    "    return loss, n_correct, n_word\n",
    "\n",
    "# Calculate cross entropy loss, apply label smoothing if needed\n",
    "def cal_loss(pred, gold, trg_pad_idx, smoothing=False):\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = gold.ne(trg_pad_idx)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, ignore_index=trg_pad_idx, reduction='sum')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def patch_src(src, pad_idx):\n",
    "    src = src.transpose(0, 1)\n",
    "    return src\n",
    "\n",
    "\n",
    "def patch_trg(trg, pad_idx):\n",
    "    trg = trg.transpose(0, 1)\n",
    "    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n",
    "    return trg, gold\n",
    "\n",
    "# Epoch operation in training phase\n",
    "def train_epoch(model, training_data, optimizer, opt, device, smoothing):\n",
    "\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n",
    "\n",
    "    desc = '  - (Training)   '\n",
    "    for batch in tqdm(training_data, mininterval=2, desc=desc, leave=False):\n",
    "\n",
    "        # prepare data\n",
    "        src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n",
    "        trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n",
    "\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(src_seq, trg_seq)\n",
    "\n",
    "        # backward and update parameters\n",
    "        loss, n_correct, n_word = cal_performance(\n",
    "            pred, gold, opt.trg_pad_idx, smoothing=smoothing) \n",
    "        loss.backward()\n",
    "        optimizer.step_and_update_lr()\n",
    "\n",
    "        # note keeping\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "# Epoch operation in evaluation phase\n",
    "def eval_epoch(model, validation_data, device, opt):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0\n",
    "\n",
    "    desc = '  - (Validation) '\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_data, mininterval=2, desc=desc, leave=False):\n",
    "\n",
    "            # prepare data\n",
    "            src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n",
    "            trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n",
    "\n",
    "            # forward\n",
    "            pred = model(src_seq, trg_seq)\n",
    "            loss, n_correct, n_word = cal_performance(\n",
    "                pred, gold, opt.trg_pad_idx, smoothing=False)\n",
    "\n",
    "            # note keeping\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "def train(model, training_data, validation_data, optimizer, device, opt):\n",
    "\n",
    "    log_train_file, log_valid_file = None, None\n",
    "\n",
    "    if opt.log:\n",
    "        log_train_file = opt.log + '.train.log'\n",
    "        log_valid_file = opt.log + '.valid.log'\n",
    "\n",
    "        print('[Info] Training performance will be written to file: {} and {}'.format(\n",
    "            log_train_file, log_valid_file))\n",
    "\n",
    "        with open(log_train_file, 'w') as log_tf, open(log_valid_file, 'w') as log_vf:\n",
    "            log_tf.write('epoch,loss,ppl,accuracy\\n')\n",
    "            log_vf.write('epoch,loss,ppl,accuracy\\n')\n",
    "\n",
    "    def print_performances(header, loss, accu, start_time):\n",
    "        print('  - {header:12} ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, '\\\n",
    "              'elapse: {elapse:3.3f} min'.format(\n",
    "                  header=f\"({header})\", ppl=math.exp(min(loss, 100)),\n",
    "                  accu=100*accu, elapse=(time.time()-start_time)/60))\n",
    "\n",
    "    #valid_accus = []\n",
    "    valid_losses = []\n",
    "    for epoch_i in range(opt.epoch):\n",
    "        print('[ Epoch', epoch_i, ']')\n",
    "\n",
    "        start = time.time()\n",
    "        train_loss, train_accu = train_epoch(\n",
    "            model, training_data, optimizer, opt, device, smoothing=opt.label_smoothing)\n",
    "        print_performances('Training', train_loss, train_accu, start)\n",
    "\n",
    "        start = time.time()\n",
    "        valid_loss, valid_accu = eval_epoch(model, validation_data, device, opt)\n",
    "        print_performances('Validation', valid_loss, valid_accu, start)\n",
    "\n",
    "        valid_losses += [valid_loss]\n",
    "\n",
    "        checkpoint = {'epoch': epoch_i, 'settings': opt, 'model': model.state_dict()}\n",
    "\n",
    "        if opt.save_model:\n",
    "            if opt.save_mode == 'all':\n",
    "                model_name = opt.save_model + '_accu_{accu:3.3f}.chkpt'.format(accu=100*valid_accu)\n",
    "                torch.save(checkpoint, model_name)\n",
    "            elif opt.save_mode == 'best':\n",
    "                model_name = opt.save_model + '.chkpt'\n",
    "                if valid_loss <= min(valid_losses):\n",
    "                    torch.save(checkpoint, model_name)\n",
    "                    print('    - [Info] The checkpoint file has been updated.')\n",
    "\n",
    "        if log_train_file and log_valid_file:\n",
    "            with open(log_train_file, 'a') as log_tf, open(log_valid_file, 'a') as log_vf:\n",
    "                log_tf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                    epoch=epoch_i, loss=train_loss,\n",
    "                    ppl=math.exp(min(train_loss, 100)), accu=100*train_accu))\n",
    "                log_vf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                    epoch=epoch_i, loss=valid_loss,\n",
    "                    ppl=math.exp(min(valid_loss, 100)), accu=100*valid_accu))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Privacy issues（Bonus）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决训练过程中对用户隐私的保护问题。\n",
    "\n",
    "由于在训练集中，我们通常会使用带有人名、手机等涉及个人隐私的数据，\n",
    "\n",
    "为了避免这些数据在回复中出现，我们需要在词向量化阶段，对训练集进行特殊的处理，即做一次数据清洗。\n",
    "\n",
    "这里我们以保护人名隐私为例，列出一些可能会用到的库：\n",
    "\n",
    "HanLP：https://github.com/hankcs/HanLP\n",
    "\n",
    "人名语料库：https://github.com/wainshine/Chinese-Names-Corpus\n",
    "\n",
    "历史名人词库：http://thuocl.thunlp.org/\n",
    "\n",
    "**过程：**\n",
    "\n",
    "1、我们对原始训练集用 HanLP 进行分词，找到所有的名词，并比对人名语料库，筛选出所有的人名的集合 A；\n",
    "\n",
    "2、我们将筛选出的人名集合 A 和历史名人语料库进行比对，筛选出非历史名人的姓名集合 B；\n",
    "\n",
    "3、我们为集合 B 中的姓名通过名字生成器，生成相应的替代姓名；\n",
    "\n",
    "4、我们将原始训练集中所有的在集合 B 中的姓名替换成相应的替代姓名；\n",
    "\n",
    "以下是简单的伪代码实现，并不代表实际可以运行，仅用来阐述思路：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "!pip install hanlp\n",
    "import hanlp\n",
    "import MoeName\n",
    "\n",
    "HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)  # 世界最大中文语料库\n",
    "\n",
    "data_path = args.data_path+args.dataset+'/'\n",
    "train_set = DialogDataset(os.path.join(data_path, 'train.h5'), config['diaglen'], config['maxlen']) \n",
    "print(\"Loaded data!\")\n",
    "\n",
    "name_set = chineseNamesCorpus(HanLP(train_set))\n",
    "name_set_no_history = name_set - THUOCL.historyName(name_set)\n",
    "name_replace = MoeName.generate(name_set_no_history.length)\n",
    "\n",
    "i = 0\n",
    "for name in name_set_no_history:\n",
    "    pos = train_set.contains(name)\n",
    "    if (pos):\n",
    "        trains_set[pos] = name_replace[i]\n",
    "    i++\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_py2",
   "language": "python",
   "name": "conda_mxnet_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
